{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoTVCUtTQL6c"
      },
      "source": [
        "# TP 1: LDA/QDA y optimización matemática de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kL_4etdeizy"
      },
      "source": [
        "# Intro teórica\n",
        "\n",
        "## Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
        "$$\n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "## Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "## LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "## Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "## Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8OF-SlPHbD"
      },
      "source": [
        "# Código provisto\n",
        "\n",
        "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PrDdJRypNB-y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.linalg as LA\n",
        "from scipy.linalg import cholesky, solve_triangular\n",
        "from scipy.linalg.lapack import dtrtri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cPL33WIN2HA"
      },
      "source": [
        "## Base code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ewg5e0hsNTQC"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: para que sirve bincount?\n",
        "    return np.log(a_priori)\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=int)\n",
        "\n",
        "    for i in range(m_obs):\n",
        "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rz2FC7A5NUpN"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
        "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: que hace axis=1? por que no axis=0?\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9lZbID0WNV1Y"
      },
      "outputs": [],
      "source": [
        "class TensorizedQDA(QDA):\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        # ask plain QDA to fit params\n",
        "        super()._fit_params(X,y)\n",
        "\n",
        "        # stack onto new dimension\n",
        "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
        "        self.tensor_means = np.stack(self.means)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "\n",
        "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        # return the class that has maximum a posteriori probability\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i-WGGi_sQ-pT"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol1(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.L_invs = [\n",
        "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L_inv = self.L_invs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = L_inv @ unbiased_x\n",
        "\n",
        "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i5DNLtYbQsHi"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol2(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.Ls = [\n",
        "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L = self.Ls[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = solve_triangular(L, unbiased_x, lower=True)\n",
        "\n",
        "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v0dRvYVQRCgc"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol3(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.L_invs = [\n",
        "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L_inv = self.L_invs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = L_inv @ unbiased_x\n",
        "\n",
        "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCtrHQDuN6R4"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rasInBMFNzUH"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "def get_penguins_dataset():\n",
        "    # get data\n",
        "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
        "\n",
        "    # drop non-numeric columns\n",
        "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
        "\n",
        "    # drop rows with missing values\n",
        "    mask = df.isna().sum(axis=1) == 0\n",
        "    df = df[mask]\n",
        "    tgt = tgt[mask]\n",
        "\n",
        "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
        "\n",
        "def get_wine_dataset():\n",
        "    # get data\n",
        "    data = load_wine()\n",
        "    X_full = data.data\n",
        "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "    return X_full, y_full\n",
        "\n",
        "def get_letters_dataset():\n",
        "    # get data\n",
        "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
        "    return letter.data, letter.target.reshape(-1,1)\n",
        "\n",
        "def label_encode(y_full):\n",
        "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
        "\n",
        "def split_transpose(X, y, test_size, random_state):\n",
        "    # X_train, X_test, y_train, y_test but all transposed\n",
        "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybPkuBdDN42P"
      },
      "source": [
        "## Benchmarking\n",
        "\n",
        "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
        "\n",
        "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
        "\n",
        "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
        "\n",
        "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nO4Py3CeNpKu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "from numpy.random import RandomState\n",
        "import tracemalloc\n",
        "\n",
        "RNG_SEED = 6553\n",
        "\n",
        "class Benchmark:\n",
        "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n = n_runs\n",
        "        self.warmup = warmup\n",
        "        self.mem_runs = mem_runs\n",
        "        self.test_sz = test_sz\n",
        "        self.det = same_splits\n",
        "        if self.det:\n",
        "            self.rng_seed = rng_seed\n",
        "        else:\n",
        "            self.rng = RandomState(rng_seed)\n",
        "\n",
        "        self.data = dict()\n",
        "\n",
        "        print(\"Benching params:\")\n",
        "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
        "        print(\"Warmup runs:\",self.warmup)\n",
        "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
        "        print(\"Running time runs:\", self.n)\n",
        "        approx_test_sz = int(self.y.size * self.test_sz)\n",
        "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
        "        print(\"Test size rows (approx):\",approx_test_sz)\n",
        "        print(\"Test size fraction:\",self.test_sz)\n",
        "\n",
        "    def bench(self, model_class, **kwargs):\n",
        "        name = model_class.__name__\n",
        "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
        "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
        "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
        "\n",
        "\n",
        "        for i in range(self.warmup):\n",
        "            # Instantiate model with error check for unsupported parameters\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            # Generate current train-test split\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "            # Run training and prediction (timing or memory measurement not recorded)\n",
        "            model.fit(X_train, y_train)\n",
        "            model.predict(X_test)\n",
        "\n",
        "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
        "\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "\n",
        "            tracemalloc.start()\n",
        "\n",
        "            t1 = time.perf_counter()\n",
        "            model.fit(X_train, y_train)\n",
        "            t2 = time.perf_counter()\n",
        "\n",
        "            _, train_peak = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.reset_peak()\n",
        "\n",
        "            model.predict(X_test)\n",
        "            t3 = time.perf_counter()\n",
        "            _, test_peak = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.stop()\n",
        "\n",
        "            mem_data[i,] = (\n",
        "                train_peak / (1024 * 1024),\n",
        "                test_peak / (1024 * 1024)\n",
        "            )\n",
        "\n",
        "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "\n",
        "            t1 = time.perf_counter()\n",
        "            model.fit(X_train, y_train)\n",
        "            t2 = time.perf_counter()\n",
        "            preds = model.predict(X_test)\n",
        "            t3 = time.perf_counter()\n",
        "\n",
        "            time_data[i,] = (\n",
        "                (t2 - t1) * 1000,\n",
        "                (t3 - t2) * 1000,\n",
        "                (y_test.flatten() == preds.flatten()).mean()\n",
        "            )\n",
        "\n",
        "        self.data[name] = (time_data, mem_data)\n",
        "\n",
        "    def summary(self, baseline=None):\n",
        "        aux = []\n",
        "        for name, (time_data, mem_data) in self.data.items():\n",
        "            result = {\n",
        "                'model': name,\n",
        "                'train_mean_ms': time_data[:, 0].mean(),\n",
        "                'train_std_ms': time_data[:, 0].std(),\n",
        "                'test_mean_ms': time_data[:, 1].mean(),\n",
        "                'test_std_ms': time_data[:, 1].std(),\n",
        "                'mean_accuracy': time_data[:, 2].mean(),\n",
        "                'train_mem_mean_mb': mem_data[:, 0].mean(),\n",
        "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
        "                'test_mem_mean_mb': mem_data[:, 1].mean(),\n",
        "                'test_mem_std_mb': mem_data[:, 1].std()\n",
        "            }\n",
        "            aux.append(result)\n",
        "        df = pd.DataFrame(aux).set_index('model')\n",
        "\n",
        "        if baseline is not None and baseline in self.data:\n",
        "            df['train_speedup'] = df.loc[baseline, 'train_mean_ms'] / df['train_mean_ms']\n",
        "            df['test_speedup'] = df.loc[baseline, 'test_mean_ms'] / df['test_mean_ms']\n",
        "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_mean_mb'] / df['train_mem_mean_mb']\n",
        "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_mean_mb'] / df['test_mem_mean_mb']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb5VEpEugFXW"
      },
      "source": [
        "## Ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLyr4-hdgJ7e",
        "outputId": "bfa9623f-2baf-4735-96b5-c714b244b8da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((178, 13), (178, 1))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
        "X_full, y_full = get_wine_dataset()\n",
        "\n",
        "X_full.shape, y_full.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxQlFUSbgYHQ",
        "outputId": "dd396038-8bab-4ebd-b981-752526d8c98c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0']], dtype='<U7'),\n",
              " array([[0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]]))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encodeamos a número las clases\n",
        "y_full_encoded = label_encode(y_full)\n",
        "\n",
        "y_full[:5], y_full_encoded[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSBNNUOmgtsI",
        "outputId": "d29b11de-b5a9-4fa3-f009-d8780104fa64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benching params:\n",
            "Total runs: 140\n",
            "Warmup runs: 20\n",
            "Peak Memory usage runs: 20\n",
            "Running time runs: 100\n",
            "Train size rows (approx): 125\n",
            "Test size rows (approx): 53\n",
            "Test size fraction: 0.3\n"
          ]
        }
      ],
      "source": [
        "# generamos el benchmark\n",
        "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
        "b = Benchmark(\n",
        "    X_full, y_full_encoded,\n",
        "    n_runs = 100,\n",
        "    warmup = 20,\n",
        "    mem_runs = 20,\n",
        "    test_sz = 0.3,\n",
        "    same_splits = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "2add89be4e944f4fbd91c5f1d459b5cb",
            "e0a05e28520841bd95e2aec19da024bd",
            "c406ae72dbf14856971c0ac3ad078062",
            "ca156cc465e1415b8df2a570abfa3ffe",
            "40416335cca64bbbad37afc46049363d",
            "4531ea3385d34454865e3c6ced188122",
            "a9bc2a66f8a84516a44d8d3ad8e885e2",
            "43ef5277fff74410bb85f09e64c37cfe",
            "d7276daf0b654112aab411ae3f14649f",
            "68a99e1ecb9741f688cb33bbfc19d5d2",
            "cec5bcb756694268abfc4eefad72f758",
            "0878ca0785b74f4fa8abce52c184ce33",
            "9cb5b6651bac47d0b99c9388def7d2e6",
            "ec4d02ef09c5492ab097ef02b9d3e2fb",
            "ed4b7065efa24e5da4f69c047647c047",
            "08dc6e2b043f409f8f8df2b63f6bc152",
            "f7bc1a202cde4351a6f98ae19393cf94",
            "caea67fec94c45a097cae720b582c68d",
            "c136c5327b804c74b0582ed04c8825dc",
            "b2d747a3bfdd4f4797f7e4b01964aa7b",
            "a5957e8514974187838fc2d43d548433",
            "38dcdaf521fe4043b7bc5fac762867e1"
          ]
        },
        "id": "zUciOjazhUu5",
        "outputId": "91fc0889-7a87-418e-9950-88371c912be9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "QDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 208.52it/s]\n",
            "QDA (TIME): 100%|██████████| 100/100 [00:00<00:00, 589.24it/s]\n"
          ]
        }
      ],
      "source": [
        "# bencheamos un par\n",
        "to_bench = [QDA]\n",
        "\n",
        "for model in to_bench:\n",
        "    b.bench(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e3704aaa731c464a8d6c63c39d0b167a",
            "ee002c5199874f8db461bb1631f198ac",
            "9e38484f037e4f86be6b768459404767",
            "d670b24d966f4de1872524684ea98840",
            "74859eadc1ef4090a07e851f5949f4d6",
            "1f6553e18f4f460ebbc4b9705cfb9c3d",
            "44cc54751afe4e618d81e92e2e64ca58",
            "f1cdc4724c6d441cbebca60f1bb11aea",
            "4965b54e63794e0b80c19c150b072eda",
            "4c09e94a9e494e6daa6f59c23b8bcb4f",
            "ab3b1739c3bf4f3ba399e3a60d043ec8",
            "6bbe1b2d6f3846168244aca7ecf761f0",
            "eb6e454a891d493b88fe82442a9288e2",
            "3fad52f5ac204004afac2df2b55cbc7b",
            "2d6622ab061342f280f7fbcbc920bc00",
            "e7cdc4320bff45f6acf901b8bd9bdf1a",
            "bf6f0511eeca467c870c6a261712ca7e",
            "26d56661a9e4490c96ef5dd831fa904b",
            "ba742120ade4495d9f75b066c43b6ffb",
            "e38ba66c86764d578b97aa127e0cd9ce",
            "f87908723b244cd99144a4e1b7be8807",
            "568fd1f1b5984675bf1716520a63b7d1"
          ]
        },
        "id": "wpPhSSCNhlvG",
        "outputId": "a5302dc3-d947-47db-d96d-cf7a98152b0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TensorizedQDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 480.44it/s]\n",
            "TensorizedQDA (TIME): 100%|██████████| 100/100 [00:00<00:00, 1099.96it/s]\n"
          ]
        }
      ],
      "source": [
        "# como es una clase, podemos seguir bencheando más después\n",
        "b.bench(TensorizedQDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "bZ5-vowshr5c",
        "outputId": "f17bc091-0cf5-42b9-cd9a-1e3d61e824b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_mean_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_mean_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.153592</td>\n",
              "      <td>0.030723</td>\n",
              "      <td>1.400253</td>\n",
              "      <td>0.057478</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>0.018348</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.008037</td>\n",
              "      <td>0.001445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.155910</td>\n",
              "      <td>0.020562</td>\n",
              "      <td>0.624067</td>\n",
              "      <td>0.033047</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.018229</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.012093</td>\n",
              "      <td>0.000213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
              "model                                                                   \n",
              "QDA                 0.153592      0.030723      1.400253     0.057478   \n",
              "TensorizedQDA       0.155910      0.020562      0.624067     0.033047   \n",
              "\n",
              "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
              "model                                                               \n",
              "QDA                 0.982407           0.018348          0.000664   \n",
              "TensorizedQDA       0.982593           0.018229          0.000698   \n",
              "\n",
              "               test_mem_mean_mb  test_mem_std_mb  \n",
              "model                                             \n",
              "QDA                    0.008037         0.001445  \n",
              "TensorizedQDA          0.012093         0.000213  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hacemos un summary\n",
        "b.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "09eKXqlXhwL-",
        "outputId": "d42734a6-6fd8-4b15-da84-144d4923113f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.153592</td>\n",
              "      <td>1.400253</td>\n",
              "      <td>0.982407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.155910</td>\n",
              "      <td>0.624067</td>\n",
              "      <td>0.982593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  test_mean_ms  mean_accuracy\n",
              "model                                                    \n",
              "QDA                 0.153592      1.400253       0.982407\n",
              "TensorizedQDA       0.155910      0.624067       0.982593"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# son muchos datos! nos quedamos con un par nomás\n",
        "summ = b.summary()\n",
        "\n",
        "# como es un pandas DataFrame, subseteamos columnas fácil\n",
        "summ[['train_mean_ms', 'test_mean_ms','mean_accuracy']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "EopB9574h8I5",
        "outputId": "c2bd86ab-3ba3-456d-c99b-d3bd131af75f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_mean_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_mean_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.153592</td>\n",
              "      <td>0.030723</td>\n",
              "      <td>1.400253</td>\n",
              "      <td>0.057478</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>0.018348</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.008037</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.155910</td>\n",
              "      <td>0.020562</td>\n",
              "      <td>0.624067</td>\n",
              "      <td>0.033047</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.018229</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.012093</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.985132</td>\n",
              "      <td>2.243754</td>\n",
              "      <td>1.006495</td>\n",
              "      <td>0.664579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
              "model                                                                   \n",
              "QDA                 0.153592      0.030723      1.400253     0.057478   \n",
              "TensorizedQDA       0.155910      0.020562      0.624067     0.033047   \n",
              "\n",
              "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
              "model                                                               \n",
              "QDA                 0.982407           0.018348          0.000664   \n",
              "TensorizedQDA       0.982593           0.018229          0.000698   \n",
              "\n",
              "               test_mem_mean_mb  test_mem_std_mb  train_speedup  test_speedup  \\\n",
              "model                                                                           \n",
              "QDA                    0.008037         0.001445       1.000000      1.000000   \n",
              "TensorizedQDA          0.012093         0.000213       0.985132      2.243754   \n",
              "\n",
              "               train_mem_reduction  test_mem_reduction  \n",
              "model                                                   \n",
              "QDA                       1.000000            1.000000  \n",
              "TensorizedQDA             1.006495            0.664579  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# podemos setear un baseline para que fabrique columnas de comparación\n",
        "summ = b.summary(baseline='QDA')\n",
        "\n",
        "summ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "z0qeE1gviFLZ",
        "outputId": "26f288da-88c0-4568-d4cb-f3d60bf5045c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.153592</td>\n",
              "      <td>1.400253</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.155910</td>\n",
              "      <td>0.624067</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.985132</td>\n",
              "      <td>2.243754</td>\n",
              "      <td>1.006495</td>\n",
              "      <td>0.664579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_mean_ms  test_mean_ms  mean_accuracy  train_speedup  \\\n",
              "model                                                                      \n",
              "QDA                 0.153592      1.400253       0.982407       1.000000   \n",
              "TensorizedQDA       0.155910      0.624067       0.982593       0.985132   \n",
              "\n",
              "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                 \n",
              "QDA                1.000000             1.000000            1.000000  \n",
              "TensorizedQDA      2.243754             1.006495            0.664579  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summ[[\n",
        "    'train_mean_ms', 'test_mean_ms','mean_accuracy',\n",
        "    'train_speedup', 'test_speedup',\n",
        "    'train_mem_reduction', 'test_mem_reduction'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF80Pck2RmaC"
      },
      "source": [
        "# Respuestas a preguntas técnicas\n",
        "\n",
        "**Q3: ¿Para que sirve bincount?**\n",
        "\n",
        "Sirve para contar la frecuencia de aparición de cada índice o valor del vector original. En este caso se lo divide además por la cantidad total de elementos del vector original con el fin de asignar una probabilidad de aparición para cada evento, siendo esta la función de densidad a priori obtenida a partir de las observaciones hechas.\n",
        "\n",
        "**Q4: ¿Por qué el `_fit_params` va al final? ¿No se puede mover a, por ejemplo, antes de la priori?**\n",
        "\n",
        "El método `_fit_params` no puede ir antes del cálculo de la distribución a priori porque el primero depende de la propia función a priori para calcular las matrices necesarias. Ejecutarlo después del momento en el que se utilice también carece de sentido ya que significaría que el modelo debe tratar de predecir valores con parámetros que no han sido entrenados, invalidando los resultados del test que se desee hacer.\n",
        "\n",
        "**Q5: ¿Por qué hace falta el flatten y no se puede directamente `X[:,y==idx]`?**\n",
        "\n",
        "La variable `y` es un objeto de tipo `np.Series` de n filas y 1 columna. El filtro que se desea utilizar en los valores del dataframe guardado en la variable `X` solo es aplicable para objetos del tipo lista, de modo que se usa el método `flatten` para transformar a `y` en una lista equivalente y poder filtrar los valores de cada observación correctamente.\n",
        "\n",
        "**Q6: ¿Por qué se usa `bias=True` en vez del default `bias=False`?**\n",
        "\n",
        "Porque se desea calcular la expresión con la hipótesis de máxima verosimilitud, que emplea un estimador sesgado que debe usar un factor $ \\frac{1}{n}$, lo cual se indica a la función configurando el parámetro `bias=True`.\n",
        "\n",
        "**Q7: ¿Qué hace `axis=1`? ¿Por qué no `axis=0`?**\n",
        "\n",
        "Para obtener el valor medio para cada observación del dataset. Hacerlo con el valor `axis=0` indicaría que se promedian los distintos features para una misma observación, lo cual no tiene sentido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Consigna QDA\n",
        "\n",
        "**Notación**: en general notamos\n",
        "\n",
        "* $k$ la cantidad de clases\n",
        "* $n$ la cantidad de observaciones\n",
        "* $p$ la cantidad de features/variables/predictores\n",
        "\n",
        "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda **fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
        "\n",
        "## Tensorización\n",
        "\n",
        "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos.\n",
        "\n",
        "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
        "\n",
        "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
        "\n",
        "Paraleliza sobre las $k$ clases. \n",
        "\n",
        "En el modelo `QDA` se crean $k$ listas de $p \\times p$ valores sobre los que luego hay que iterar uno por uno. En cambio `TensorizedQDA` usa un tensor de dimensión $k \\times n \\times n$ que permite hacer cálculos directamente, paralelizando los cálculos y evitando hacer las $k$ iteraciones hechas en el método `_predict_one`.\n",
        "\n",
        "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
        "\n",
        "Como se introdujo antes, `TensorizedQDA` usa un tensor de dimensión $k \\times p \\times p$ para la inversa de sus covarianzas, que tiene tamaño $p \\times p$; y de $k \\times p \\times 1$ para sus medias. \n",
        "\n",
        "El cálculo hecho en los métodos `_predict_one` es exactamente el mismo, pero en la clase `QDA` se debe calcular el logaritmo de la probabilidad condicional (hecho en el método individualmente para cada clase `_predict_log_conditionals`) mientras que el modelo `TensorizedQDA` puede hacerlo con las capacidades de cálculo tensorial de un array de numpy de dimensión 3 y calcular el logaritmo de todas las clases a la vez.\n",
        "\n",
        "Se ve que las predicciones se realizan para cada punto de $x$ que se tiene, pero a la forma de los tensores de medias y covarianzas es se les agrega una dimensión adicional, que corresponde a la categoría que se esta analizando.\n",
        "\n",
        "Esto llega a predecir lo mismo que QDA debido a que `numpy` utiliza una regla de multiplicación de matrices conocido como *broadcasting* donde en este caso está utilizando la primer dimensión correspondiente a las categorías como dimensión principal, para después propagar las operaciones matriciales en las dimensiones restantes de la matriz. Es como si se hiciera $k$ veces la misma operación de matriz, para cada una de las diferentes categorías.\n",
        "\n",
        "### 2) Optimización\n",
        "\n",
        "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
        "\n",
        "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict.\n",
        "\n",
        "La clave en este punto es comprender que la expresión a la que llegamos para la predicción y actualización de la verosimilitud:\n",
        "\n",
        "$$ (x_s-\\mu_c)^T \\Sigma_c^{-1} (x_s-\\mu_c) $$\n",
        "\n",
        "Permite determinar la similitud entre dos valores aleatorios de igual distribución de probabilidad conociendo su matriz de covarianza. Esto permite analizar los valores que minimizan ese cálculo (usando para ello la log-verosimilitud desarrollado al inicio del notebook). Esta distancia es llamada **distancia de Mahalanobis**, y su desarrollo usando operaciones tensoriales se puede lograr simplemente multiplicando tensores o fijando algunas variables y haciendo el producto interno de los vectores no fijos en lo que se conoce como una \"suma de Einstein\": "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FasterQDA(TensorizedQDA):\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.n_samples = X.shape[1]\n",
        "        self.n_classes = len(self.log_a_priori)\n",
        "\n",
        "        # Make room for the n_classes dimension on the tensor (1, n_features, n_samples)\n",
        "        X_expanded = X[np.newaxis, :, :]\n",
        "\n",
        "        # Repeat all mean matrix for every sample into a single tensor (n_classes, n_features, n_samples)\n",
        "        self.means_expanded = self.tensor_means @ np.ones((1, self.n_samples))\n",
        "\n",
        "        # log_a_priori: (n_classes,) -> broadcast to (n_classes, n_samples)\n",
        "        total_log_posterior = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X_expanded)\n",
        "\n",
        "        # Pick class with maximum log posterior for each sample\n",
        "        y_hat = np.argmax(total_log_posterior, axis=0)\n",
        "\n",
        "        return y_hat.reshape(1, -1)\n",
        "    \n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.means_expanded\n",
        "\n",
        "        # Transpose features & samples to multiply tensors (n_classes, n_samples, n_features)\n",
        "        unbiased_x_t = unbiased_x.transpose(0, 2, 1)\n",
        "\n",
        "        # Use matmul to solve the Mahalanobis expression: (x_s-mu_c).T*Sigma_c^{-1}*(x_s-mu_c)\n",
        "        # Sigma is (n_classes, n_features, n_features)\n",
        "        temp = np.matmul(unbiased_x_t, self.tensor_inv_cov)  # (n_classes, n_samples, n_features)\n",
        "        prod = np.matmul(temp, unbiased_x)  # (n_classes, n_samples, n_samples)\n",
        "\n",
        "        # Diagonalize without using for loops\n",
        "        diag_inner_prod = np.sum(np.multiply(prod, np.eye(self.n_samples)[np.newaxis, :, :]), axis=2)\n",
        "\n",
        "        log_det = np.log(np.linalg.det(self.tensor_inv_cov))\n",
        "        return 0.5*log_det[:, np.newaxis] - 0.5 * diag_inner_prod\n",
        "    \n",
        "class FasterQDA_Einstein(TensorizedQDA):\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[1]\n",
        "\n",
        "        # Make room for the n_classes dimension on the tensor (1, n_features, n_samples)\n",
        "        X_expanded = X[np.newaxis, :, :]\n",
        "\n",
        "        # Repeat all mean matrix for every sample into a single tensor (n_classes, n_features, n_samples)\n",
        "        self.means_expanded = self.tensor_means @ np.ones((1, n_samples))\n",
        "\n",
        "        # log_a_priori: (n_classes,) -> broadcast to (n_classes, n_samples)\n",
        "        total_log_posterior = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X_expanded)\n",
        "\n",
        "        # Pick class with maximum log posterior for each sample\n",
        "        y_hat = np.argmax(total_log_posterior, axis=0)\n",
        "\n",
        "        return y_hat.reshape(1, -1)\n",
        "    \n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.means_expanded\n",
        "\n",
        "        # Use einsum to multiply tensors. (x_s-mu_c).T*Sigma_c^{-1}*(x_s-mu_c). This expression is already diagonalized\n",
        "        # Sigma is (n_classes, n_features, n_features)\n",
        "        inner_prod = np.einsum('kpn,kpq,kqn->kn', unbiased_x, self.tensor_inv_cov, unbiased_x)\n",
        "\n",
        "        log_det = np.log(np.linalg.det(self.tensor_inv_cov))\n",
        "        return 0.5*log_det[:, np.newaxis] - 0.5 * inner_prod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.\n",
        "\n",
        "Como se mencionó antes, la expresión de Mahalanois $(x_s-\\mu_c)^T \\Sigma_c^{-1} (x_s-\\mu_c)$ resulta en una matriz auxiliar de $n \\times n$ que no es utilizada en su totalidad cuando se diagonaliza. Por este motivo se propone el modelo alternativo `FasterQDA_Einstein` que usa sumas de Einstein para calcular los términos de la diagonal sin crear la matriz completa, con la esperanza de acelerar los cálculos.\n",
        "\n",
        "5. Demostrar que\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ \n",
        "\n",
        "es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
        "\n",
        "$$\n",
        "np.sum(A^T \\odot B, axis=0).T\n",
        "$$\n",
        "\n",
        "queda a preferencia del alumno cuál usar.\n",
        "\n",
        "Por definición, un elemento de una matriz se resuelve como:\n",
        "\n",
        "$$ C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj} $$\n",
        "\n",
        "Particularmente, la diagonal de la matriz es el elemento donde $i=j$, de modo que:\n",
        "\n",
        "$$ \\mathrm{diag}(AB)_i = C_{ii} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{ki} = \\sum_{k=1}^{n} A_{ik} \\cdot B^T_{ik} = \\sum_{k=1}^{n} A_i \\odot B_i^T $$\n",
        "\n",
        "Generalizando para la diagonal entera en lugar de solo un elemento de ella:\n",
        "\n",
        "$$ \\mathrm{diag}(AB) = \\sum_{k=1}^{n} A \\odot B^T $$\n",
        "\n",
        "Empíricamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Igualdad en ambos lados de la ecuación:  True\n"
          ]
        }
      ],
      "source": [
        "n, p = 100, 20\n",
        "A = np.random.randn(n, p)\n",
        "B = np.random.randn(p, n)\n",
        "\n",
        "left = np.diag(A @ B)\n",
        "right = np.sum(A * B.T, axis=1)\n",
        "\n",
        "print(\"Igualdad en ambos lados de la ecuación: \", np.allclose(left, right))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientQDA(TensorizedQDA):\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[1]\n",
        "\n",
        "        # Make room for the n_classes dimension on the tensor (1, n_features, n_samples)\n",
        "        X_expanded = X[np.newaxis, :, :]\n",
        "\n",
        "        # Repeat all mean matrix for every sample into a single tensor (n_classes, n_features, n_samples)\n",
        "        self.means_expanded = self.tensor_means @ np.ones((1, n_samples))\n",
        "\n",
        "        # log_a_priori: (n_classes,) -> broadcast to (n_classes, n_samples)\n",
        "        total_log_posterior = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X_expanded)\n",
        "\n",
        "        # Pick class with maximum log posterior for each sample\n",
        "        y_hat = np.argmax(total_log_posterior, axis=0)\n",
        "\n",
        "        return y_hat.reshape(1, -1)\n",
        "    \n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.means_expanded\n",
        "\n",
        "        # Use einsum to multiply tensors. (x_s-mu_c).T*Sigma_c^{-1}\n",
        "        # Sigma is (n_classes, n_features, n_features)\n",
        "        A = np.einsum(\"kpn,kpq->kqn\", unbiased_x, self.tensor_inv_cov)\n",
        "\n",
        "        # Diagonalize the expression using the inner product property to avoid creating a nxn matrix\n",
        "        inner_prod = np.sum(A * unbiased_x, axis=1)\n",
        "\n",
        "        log_det = np.log(np.linalg.det(self.tensor_inv_cov))\n",
        "        return 0.5*log_det[:, np.newaxis] - 0.5 * inner_prod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benching params:\n",
            "Total runs: 10040\n",
            "Warmup runs: 20\n",
            "Peak Memory usage runs: 20\n",
            "Running time runs: 10000\n",
            "Train size rows (approx): 125\n",
            "Test size rows (approx): 53\n",
            "Test size fraction: 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "QDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 189.71it/s]\n",
            "QDA (TIME): 100%|██████████| 10000/10000 [00:16<00:00, 600.49it/s]\n",
            "TensorizedQDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 473.83it/s]\n",
            "TensorizedQDA (TIME): 100%|██████████| 10000/10000 [00:08<00:00, 1118.18it/s]\n",
            "FasterQDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 1581.89it/s]\n",
            "FasterQDA (TIME): 100%|██████████| 10000/10000 [00:02<00:00, 3363.68it/s]\n",
            "FasterQDA_Einstein (MEM): 100%|██████████| 20/20 [00:00<00:00, 1566.30it/s]\n",
            "FasterQDA_Einstein (TIME): 100%|██████████| 10000/10000 [00:03<00:00, 3331.83it/s]\n",
            "EfficientQDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 1544.35it/s]\n",
            "EfficientQDA (TIME): 100%|██████████| 10000/10000 [00:03<00:00, 3256.57it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.142851</td>\n",
              "      <td>1.389113</td>\n",
              "      <td>0.983828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.148373</td>\n",
              "      <td>0.620362</td>\n",
              "      <td>0.983635</td>\n",
              "      <td>0.962779</td>\n",
              "      <td>2.239197</td>\n",
              "      <td>1.016405</td>\n",
              "      <td>0.642755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA</th>\n",
              "      <td>0.136503</td>\n",
              "      <td>0.048445</td>\n",
              "      <td>0.983680</td>\n",
              "      <td>1.046504</td>\n",
              "      <td>28.674314</td>\n",
              "      <td>1.033720</td>\n",
              "      <td>0.028011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA_Einstein</th>\n",
              "      <td>0.137026</td>\n",
              "      <td>0.049832</td>\n",
              "      <td>0.983781</td>\n",
              "      <td>1.042513</td>\n",
              "      <td>27.875808</td>\n",
              "      <td>1.028427</td>\n",
              "      <td>0.026414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EfficientQDA</th>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.056599</td>\n",
              "      <td>0.983863</td>\n",
              "      <td>1.043510</td>\n",
              "      <td>24.543112</td>\n",
              "      <td>1.032311</td>\n",
              "      <td>0.031544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    train_mean_ms  test_mean_ms  mean_accuracy  train_speedup  \\\n",
              "model                                                                           \n",
              "QDA                      0.142851      1.389113       0.983828       1.000000   \n",
              "TensorizedQDA            0.148373      0.620362       0.983635       0.962779   \n",
              "FasterQDA                0.136503      0.048445       0.983680       1.046504   \n",
              "FasterQDA_Einstein       0.137026      0.049832       0.983781       1.042513   \n",
              "EfficientQDA             0.136895      0.056599       0.983863       1.043510   \n",
              "\n",
              "                    test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                      \n",
              "QDA                     1.000000             1.000000            1.000000  \n",
              "TensorizedQDA           2.239197             1.016405            0.642755  \n",
              "FasterQDA              28.674314             1.033720            0.028011  \n",
              "FasterQDA_Einstein     27.875808             1.028427            0.026414  \n",
              "EfficientQDA           24.543112             1.032311            0.031544  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b = Benchmark(\n",
        "    X_full, y_full_encoded,\n",
        "    n_runs = 10000,\n",
        "    warmup = 20,\n",
        "    mem_runs = 20,\n",
        "    test_sz = 0.3,\n",
        "    same_splits = False\n",
        ")\n",
        "\n",
        "to_bench = [QDA, TensorizedQDA, FasterQDA, FasterQDA_Einstein, EfficientQDA]\n",
        "\n",
        "for model in to_bench:\n",
        "    b.bench(model)\n",
        "\n",
        "summ = b.summary(baseline='QDA')\n",
        "summ[[\n",
        "    'train_mean_ms', 'test_mean_ms','mean_accuracy',\n",
        "    'train_speedup', 'test_speedup',\n",
        "    'train_mem_reduction', 'test_mem_reduction'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se observa que el modelo `FasterQDA` permite entrenar el modelo ligeramente más rápido (posiblemente por efectos computacionales, ya que el entrenamiento no cambia), pero se destaca que tiene una respuesta muchísimo más rápida a costa de mucha mayor utilización de memoria. \n",
        "\n",
        "En el modelo `FasterQDA_Einstein` propuesto se mejora aún más la velocidad de respuesta utilizando algo más de memoria y mejorando ligeramente la certeza con respecto al modelo evaluado en el párrafo anterior.\n",
        "\n",
        "Por último, `EfficientQDA` reduce el tiempo de respuesta con respecto a los modelos anteriores manteniendo similar su certeza, pero conserva ligeramente más la memoria utilizada para ello.\n",
        "\n",
        "En líneas generales el experimento entrega resultados muy satisfactorios, acelerando muchísimo la respuesta del modelo si se dispone de los recursos de hardware necesarios y sin empeorar la calidad de la misma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky\n",
        "\n",
        "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
        "\n",
        "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
        "\n",
        "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
        "\n",
        "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
        "\n",
        "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
        "\n",
        "$$ A=LL^T \\Rightarrow A^{-1} = (LL^T)^{-1} = (L^T)^{-1} L^{-1} $$\n",
        "\n",
        "Dado que la matriz de covarianzas $\\Sigma$ es **siempre simétrica**, es muy fácil descomponerla en términos de una matriz triangular y por lo tanto puede aplicarse la descomposición de Cholesky sin problema. Esto permite que, al calcular la distancia de Mahalanobis, si consideramos $z=(x_s-\\mu_c)$:\n",
        "\n",
        "$$ z^T \\Sigma_c^{-1} z \\Rightarrow z^T (L^T)^{-1} L^{-1} z $$\n",
        "\n",
        "Si ahora se resuelve una parte de la ecuación considerando $y=L^{-1} z$, se puede también considerar $y^T=z^T (L^T)^{-1} $; de modo que el cálculo de la distancia de Mahalanois se puede expresar ahora como:\n",
        "\n",
        "$$ y^T y $$\n",
        "\n",
        "De modo que es posible resolver únicamente $ y = L^{-1} (x_s-\\mu_c) $ y trasponer el resultado para obtener una expresión equivalente que acelera mucho los cálculos.\n",
        "\n",
        "9.  Explicar las diferencias entre `QDA_Chol1` y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
        "\n",
        "La diferencia principal es el uso de la factorización de Cholesky que se describió en el punto anterior en lugar del cálculo directo de la inversa de la matriz de covarianzas.\n",
        "\n",
        "Para el cálculo del logaritmo condicional previene el uso de funciones inversas y productos complejos entre matrices, acelerando los cálculos y evitando inestabilidades por las funciones matemáticas usadas. Además por propiedades de las matrices triangulares, la operación matricial para el cálculo del logaritmo de verosimilitud puede reemplazarse por la **sumatoria del cuadrado de $y$**, y la operacion de determinante puede reemplazarse por el **producto de los elementos de la diagonal**.\n",
        "    \n",
        "10. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
        "\n",
        "Difieren en que guardan el valor de L en lugar de su inversa (`QDA_Chol2`) o cómo se calcula la inversa de manera eficiente usando `scipy.linalg.lapack.dtrtri` (`QDA_Chol3`). En la predicción, la única diferencia es con `QDA_Chol2`, que usa el método `scipy.linalg.solve_triangular` para calcular el logaritmo de verosimilitud de manera más eficiente.\n",
        "    \n",
        "11. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás? ¿Alguna que sea peor?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "QDA_Chol1 (MEM): 100%|██████████| 20/20 [00:00<00:00, 290.07it/s]\n",
            "QDA_Chol1 (TIME): 100%|██████████| 10000/10000 [00:11<00:00, 884.12it/s]\n",
            "QDA_Chol2 (MEM): 100%|██████████| 20/20 [00:00<00:00, 189.19it/s]\n",
            "QDA_Chol2 (TIME): 100%|██████████| 10000/10000 [00:19<00:00, 508.53it/s]\n",
            "QDA_Chol3 (MEM): 100%|██████████| 20/20 [00:00<00:00, 327.96it/s]\n",
            "QDA_Chol3 (TIME): 100%|██████████| 10000/10000 [00:10<00:00, 924.33it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.142851</td>\n",
              "      <td>1.389113</td>\n",
              "      <td>0.983828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.148373</td>\n",
              "      <td>0.620362</td>\n",
              "      <td>0.983635</td>\n",
              "      <td>0.962779</td>\n",
              "      <td>2.239197</td>\n",
              "      <td>1.016405</td>\n",
              "      <td>0.642755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA</th>\n",
              "      <td>0.136503</td>\n",
              "      <td>0.048445</td>\n",
              "      <td>0.983680</td>\n",
              "      <td>1.046504</td>\n",
              "      <td>28.674314</td>\n",
              "      <td>1.033720</td>\n",
              "      <td>0.028011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA_Einstein</th>\n",
              "      <td>0.137026</td>\n",
              "      <td>0.049832</td>\n",
              "      <td>0.983781</td>\n",
              "      <td>1.042513</td>\n",
              "      <td>27.875808</td>\n",
              "      <td>1.028427</td>\n",
              "      <td>0.026414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EfficientQDA</th>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.056599</td>\n",
              "      <td>0.983863</td>\n",
              "      <td>1.043510</td>\n",
              "      <td>24.543112</td>\n",
              "      <td>1.032311</td>\n",
              "      <td>0.031544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QDA_Chol1</th>\n",
              "      <td>0.161412</td>\n",
              "      <td>0.845809</td>\n",
              "      <td>0.983670</td>\n",
              "      <td>0.885008</td>\n",
              "      <td>1.642349</td>\n",
              "      <td>1.020523</td>\n",
              "      <td>1.005820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QDA_Chol2</th>\n",
              "      <td>0.120216</td>\n",
              "      <td>1.720458</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>1.188281</td>\n",
              "      <td>0.807409</td>\n",
              "      <td>1.031943</td>\n",
              "      <td>1.014498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QDA_Chol3</th>\n",
              "      <td>0.123032</td>\n",
              "      <td>0.840302</td>\n",
              "      <td>0.983694</td>\n",
              "      <td>1.161084</td>\n",
              "      <td>1.653112</td>\n",
              "      <td>1.043376</td>\n",
              "      <td>1.025312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    train_mean_ms  test_mean_ms  mean_accuracy  train_speedup  \\\n",
              "model                                                                           \n",
              "QDA                      0.142851      1.389113       0.983828       1.000000   \n",
              "TensorizedQDA            0.148373      0.620362       0.983635       0.962779   \n",
              "FasterQDA                0.136503      0.048445       0.983680       1.046504   \n",
              "FasterQDA_Einstein       0.137026      0.049832       0.983781       1.042513   \n",
              "EfficientQDA             0.136895      0.056599       0.983863       1.043510   \n",
              "QDA_Chol1                0.161412      0.845809       0.983670       0.885008   \n",
              "QDA_Chol2                0.120216      1.720458       0.983498       1.188281   \n",
              "QDA_Chol3                0.123032      0.840302       0.983694       1.161084   \n",
              "\n",
              "                    test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                      \n",
              "QDA                     1.000000             1.000000            1.000000  \n",
              "TensorizedQDA           2.239197             1.016405            0.642755  \n",
              "FasterQDA              28.674314             1.033720            0.028011  \n",
              "FasterQDA_Einstein     27.875808             1.028427            0.026414  \n",
              "EfficientQDA           24.543112             1.032311            0.031544  \n",
              "QDA_Chol1               1.642349             1.020523            1.005820  \n",
              "QDA_Chol2               0.807409             1.031943            1.014498  \n",
              "QDA_Chol3               1.653112             1.043376            1.025312  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_bench = [QDA_Chol1, QDA_Chol2, QDA_Chol3]\n",
        "\n",
        "for model in to_bench:\n",
        "    b.bench(model)\n",
        "\n",
        "summ = b.summary(baseline='QDA')\n",
        "summ[[\n",
        "    'train_mean_ms', 'test_mean_ms','mean_accuracy',\n",
        "    'train_speedup', 'test_speedup',\n",
        "    'train_mem_reduction', 'test_mem_reduction'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todos los métodos se mantienen similares en cuanto a certeza, indicando que los cálculos realizados por el momento son todos equivalentes. El modelo `QDA_Chol1` empeora los tiempos de entrenamiento respecto a `QDA`, pero los modelos sucesores `QDA_Chol2` y `QDA_Chol3` obtienen aceleraciones significativas. En su uso con los datasets de validación si bien se observan mejoras respecto al `QDA` original no alcanza las elevadas velocidades de los modelos evaluados previamente, pero sí conservan mejor la memoria utilizada. El modelo `QDA_Chol3` es el que mejor performa bajo estos parámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Optimización\n",
        "\n",
        "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorizedChol(QDA_Chol3):\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        # ask plain QDA to fit params\n",
        "        super()._fit_params(X,y)\n",
        "\n",
        "        # stack onto new dimension\n",
        "        self.tensor_L_invs = np.stack(self.L_invs)\n",
        "        self.tensor_means = np.stack(self.means)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        y = np.matmul(self.tensor_L_invs, unbiased_x)\n",
        "\n",
        "        return -np.log(np.diagonal(self.tensor_L_invs, axis1=1, axis2=2)).sum(axis=1) - 0.5 * (y**2).sum(axis=1).squeeze(-1)\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        # return the class that has maximum a posteriori probability\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientChol(TensorizedChol):\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[1]\n",
        "\n",
        "        # Make room for the n_classes dimension on the tensor (1, n_features, n_samples)\n",
        "        X_expanded = X[np.newaxis, :, :]\n",
        "\n",
        "        # Repeat all mean matrix for every sample into a single tensor (n_classes, n_features, n_samples)\n",
        "        self.means_expanded = self.tensor_means @ np.ones((1, n_samples))\n",
        "\n",
        "        # log_a_priori: (n_classes,) -> broadcast to (n_classes, n_samples)\n",
        "        total_log_posterior = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X_expanded)\n",
        "\n",
        "        # Pick class with maximum log posterior for each sample\n",
        "        y_hat = np.argmax(total_log_posterior, axis=0)\n",
        "\n",
        "        return y_hat.reshape(1, -1)\n",
        "    \n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.means_expanded\n",
        "        y = np.einsum('cij,cjk->cik', self.tensor_L_invs, unbiased_x)\n",
        "\n",
        "        log_det = -np.log(np.diagonal(self.tensor_L_invs, axis1=1, axis2=2)).sum(axis=1)\n",
        "        log_det = log_det[:, np.newaxis]\n",
        "        return log_det - 0.5 * (y**2).sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "14. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TensorizedChol (MEM): 100%|██████████| 20/20 [00:00<00:00, 495.65it/s]\n",
            "TensorizedChol (TIME): 100%|██████████| 10000/10000 [00:07<00:00, 1336.73it/s]\n",
            "EfficientChol (MEM): 100%|██████████| 20/20 [00:00<00:00, 1678.59it/s]\n",
            "EfficientChol (TIME): 100%|██████████| 10000/10000 [00:02<00:00, 3517.53it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mean_ms</th>\n",
              "      <th>test_mean_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.142851</td>\n",
              "      <td>1.389113</td>\n",
              "      <td>0.983828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.148373</td>\n",
              "      <td>0.620362</td>\n",
              "      <td>0.983635</td>\n",
              "      <td>0.962779</td>\n",
              "      <td>2.239197</td>\n",
              "      <td>1.016405</td>\n",
              "      <td>0.642755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA</th>\n",
              "      <td>0.136503</td>\n",
              "      <td>0.048445</td>\n",
              "      <td>0.983680</td>\n",
              "      <td>1.046504</td>\n",
              "      <td>28.674314</td>\n",
              "      <td>1.033720</td>\n",
              "      <td>0.028011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA_Einstein</th>\n",
              "      <td>0.137026</td>\n",
              "      <td>0.049832</td>\n",
              "      <td>0.983781</td>\n",
              "      <td>1.042513</td>\n",
              "      <td>27.875808</td>\n",
              "      <td>1.028427</td>\n",
              "      <td>0.026414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EfficientQDA</th>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.056599</td>\n",
              "      <td>0.983863</td>\n",
              "      <td>1.043510</td>\n",
              "      <td>24.543112</td>\n",
              "      <td>1.032311</td>\n",
              "      <td>0.031544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QDA_Chol1</th>\n",
              "      <td>0.161412</td>\n",
              "      <td>0.845809</td>\n",
              "      <td>0.983670</td>\n",
              "      <td>0.885008</td>\n",
              "      <td>1.642349</td>\n",
              "      <td>1.020523</td>\n",
              "      <td>1.005820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QDA_Chol2</th>\n",
              "      <td>0.120216</td>\n",
              "      <td>1.720458</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>1.188281</td>\n",
              "      <td>0.807409</td>\n",
              "      <td>1.031943</td>\n",
              "      <td>1.014498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QDA_Chol3</th>\n",
              "      <td>0.123032</td>\n",
              "      <td>0.840302</td>\n",
              "      <td>0.983694</td>\n",
              "      <td>1.161084</td>\n",
              "      <td>1.653112</td>\n",
              "      <td>1.043376</td>\n",
              "      <td>1.025312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedChol</th>\n",
              "      <td>0.133383</td>\n",
              "      <td>0.495730</td>\n",
              "      <td>0.938281</td>\n",
              "      <td>1.070986</td>\n",
              "      <td>2.802155</td>\n",
              "      <td>1.038708</td>\n",
              "      <td>0.608855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EfficientChol</th>\n",
              "      <td>0.124889</td>\n",
              "      <td>0.049573</td>\n",
              "      <td>0.938465</td>\n",
              "      <td>1.143825</td>\n",
              "      <td>28.021470</td>\n",
              "      <td>1.027481</td>\n",
              "      <td>0.031568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    train_mean_ms  test_mean_ms  mean_accuracy  train_speedup  \\\n",
              "model                                                                           \n",
              "QDA                      0.142851      1.389113       0.983828       1.000000   \n",
              "TensorizedQDA            0.148373      0.620362       0.983635       0.962779   \n",
              "FasterQDA                0.136503      0.048445       0.983680       1.046504   \n",
              "FasterQDA_Einstein       0.137026      0.049832       0.983781       1.042513   \n",
              "EfficientQDA             0.136895      0.056599       0.983863       1.043510   \n",
              "QDA_Chol1                0.161412      0.845809       0.983670       0.885008   \n",
              "QDA_Chol2                0.120216      1.720458       0.983498       1.188281   \n",
              "QDA_Chol3                0.123032      0.840302       0.983694       1.161084   \n",
              "TensorizedChol           0.133383      0.495730       0.938281       1.070986   \n",
              "EfficientChol            0.124889      0.049573       0.938465       1.143825   \n",
              "\n",
              "                    test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                      \n",
              "QDA                     1.000000             1.000000            1.000000  \n",
              "TensorizedQDA           2.239197             1.016405            0.642755  \n",
              "FasterQDA              28.674314             1.033720            0.028011  \n",
              "FasterQDA_Einstein     27.875808             1.028427            0.026414  \n",
              "EfficientQDA           24.543112             1.032311            0.031544  \n",
              "QDA_Chol1               1.642349             1.020523            1.005820  \n",
              "QDA_Chol2               0.807409             1.031943            1.014498  \n",
              "QDA_Chol3               1.653112             1.043376            1.025312  \n",
              "TensorizedChol          2.802155             1.038708            0.608855  \n",
              "EfficientChol          28.021470             1.027481            0.031568  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_bench = [TensorizedChol, EfficientChol]\n",
        "\n",
        "for model in to_bench:\n",
        "    b.bench(model)\n",
        "\n",
        "summ = b.summary(baseline='QDA')\n",
        "summ[[\n",
        "    'train_mean_ms', 'test_mean_ms','mean_accuracy',\n",
        "    'train_speedup', 'test_speedup',\n",
        "    'train_mem_reduction', 'test_mem_reduction'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se observa que los nuevos modelos combinan la rapidez de los modelos individuales vistos anteriormente, acelerando el tiempo de ambos conjuntos, entrenamiento y evaluación, sin ningún coste adicional en memoria. Sin embargo sí se observa una reducción en la certeza que puede resultar muy perjudicial según la aplicación a la que se destine el modelo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0878ca0785b74f4fa8abce52c184ce33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cb5b6651bac47d0b99c9388def7d2e6",
              "IPY_MODEL_ec4d02ef09c5492ab097ef02b9d3e2fb",
              "IPY_MODEL_ed4b7065efa24e5da4f69c047647c047"
            ],
            "layout": "IPY_MODEL_08dc6e2b043f409f8f8df2b63f6bc152"
          }
        },
        "08dc6e2b043f409f8f8df2b63f6bc152": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f6553e18f4f460ebbc4b9705cfb9c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d56661a9e4490c96ef5dd831fa904b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2add89be4e944f4fbd91c5f1d459b5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0a05e28520841bd95e2aec19da024bd",
              "IPY_MODEL_c406ae72dbf14856971c0ac3ad078062",
              "IPY_MODEL_ca156cc465e1415b8df2a570abfa3ffe"
            ],
            "layout": "IPY_MODEL_40416335cca64bbbad37afc46049363d"
          }
        },
        "2d6622ab061342f280f7fbcbc920bc00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87908723b244cd99144a4e1b7be8807",
            "placeholder": "​",
            "style": "IPY_MODEL_568fd1f1b5984675bf1716520a63b7d1",
            "value": " 100/100 [00:00&lt;00:00, 180.63it/s]"
          }
        },
        "38dcdaf521fe4043b7bc5fac762867e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fad52f5ac204004afac2df2b55cbc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba742120ade4495d9f75b066c43b6ffb",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e38ba66c86764d578b97aa127e0cd9ce",
            "value": 100
          }
        },
        "40416335cca64bbbad37afc46049363d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ef5277fff74410bb85f09e64c37cfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44cc54751afe4e618d81e92e2e64ca58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4531ea3385d34454865e3c6ced188122": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4965b54e63794e0b80c19c150b072eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c09e94a9e494e6daa6f59c23b8bcb4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568fd1f1b5984675bf1716520a63b7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a99e1ecb9741f688cb33bbfc19d5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bbe1b2d6f3846168244aca7ecf761f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb6e454a891d493b88fe82442a9288e2",
              "IPY_MODEL_3fad52f5ac204004afac2df2b55cbc7b",
              "IPY_MODEL_2d6622ab061342f280f7fbcbc920bc00"
            ],
            "layout": "IPY_MODEL_e7cdc4320bff45f6acf901b8bd9bdf1a"
          }
        },
        "74859eadc1ef4090a07e851f5949f4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb5b6651bac47d0b99c9388def7d2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7bc1a202cde4351a6f98ae19393cf94",
            "placeholder": "​",
            "style": "IPY_MODEL_caea67fec94c45a097cae720b582c68d",
            "value": "QDA (TIME): 100%"
          }
        },
        "9e38484f037e4f86be6b768459404767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cdc4724c6d441cbebca60f1bb11aea",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4965b54e63794e0b80c19c150b072eda",
            "value": 20
          }
        },
        "a5957e8514974187838fc2d43d548433": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9bc2a66f8a84516a44d8d3ad8e885e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab3b1739c3bf4f3ba399e3a60d043ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2d747a3bfdd4f4797f7e4b01964aa7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba742120ade4495d9f75b066c43b6ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6f0511eeca467c870c6a261712ca7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c136c5327b804c74b0582ed04c8825dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c406ae72dbf14856971c0ac3ad078062": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ef5277fff74410bb85f09e64c37cfe",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7276daf0b654112aab411ae3f14649f",
            "value": 20
          }
        },
        "ca156cc465e1415b8df2a570abfa3ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a99e1ecb9741f688cb33bbfc19d5d2",
            "placeholder": "​",
            "style": "IPY_MODEL_cec5bcb756694268abfc4eefad72f758",
            "value": " 20/20 [00:00&lt;00:00, 37.71it/s]"
          }
        },
        "caea67fec94c45a097cae720b582c68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cec5bcb756694268abfc4eefad72f758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d670b24d966f4de1872524684ea98840": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c09e94a9e494e6daa6f59c23b8bcb4f",
            "placeholder": "​",
            "style": "IPY_MODEL_ab3b1739c3bf4f3ba399e3a60d043ec8",
            "value": " 20/20 [00:00&lt;00:00, 88.29it/s]"
          }
        },
        "d7276daf0b654112aab411ae3f14649f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0a05e28520841bd95e2aec19da024bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4531ea3385d34454865e3c6ced188122",
            "placeholder": "​",
            "style": "IPY_MODEL_a9bc2a66f8a84516a44d8d3ad8e885e2",
            "value": "QDA (MEM): 100%"
          }
        },
        "e3704aaa731c464a8d6c63c39d0b167a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee002c5199874f8db461bb1631f198ac",
              "IPY_MODEL_9e38484f037e4f86be6b768459404767",
              "IPY_MODEL_d670b24d966f4de1872524684ea98840"
            ],
            "layout": "IPY_MODEL_74859eadc1ef4090a07e851f5949f4d6"
          }
        },
        "e38ba66c86764d578b97aa127e0cd9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7cdc4320bff45f6acf901b8bd9bdf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6e454a891d493b88fe82442a9288e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6f0511eeca467c870c6a261712ca7e",
            "placeholder": "​",
            "style": "IPY_MODEL_26d56661a9e4490c96ef5dd831fa904b",
            "value": "TensorizedQDA (TIME): 100%"
          }
        },
        "ec4d02ef09c5492ab097ef02b9d3e2fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c136c5327b804c74b0582ed04c8825dc",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2d747a3bfdd4f4797f7e4b01964aa7b",
            "value": 100
          }
        },
        "ed4b7065efa24e5da4f69c047647c047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5957e8514974187838fc2d43d548433",
            "placeholder": "​",
            "style": "IPY_MODEL_38dcdaf521fe4043b7bc5fac762867e1",
            "value": " 100/100 [00:00&lt;00:00, 114.33it/s]"
          }
        },
        "ee002c5199874f8db461bb1631f198ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f6553e18f4f460ebbc4b9705cfb9c3d",
            "placeholder": "​",
            "style": "IPY_MODEL_44cc54751afe4e618d81e92e2e64ca58",
            "value": "TensorizedQDA (MEM): 100%"
          }
        },
        "f1cdc4724c6d441cbebca60f1bb11aea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7bc1a202cde4351a6f98ae19393cf94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87908723b244cd99144a4e1b7be8807": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
